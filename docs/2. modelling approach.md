# Modelling Approach

## Overview

The name classification system uses a **hybrid machine learning approach** that combines domain-specific feature engineering with character-level pattern recognition, semantic embeddings, and a Random Forest classifier. This approach achieves 93.14% test accuracy while maintaining fast inference (<10ms) on CPU-only hardware.

## Feature Iterations

I built the feature set in four iterations, progressively adding complexity:

**Iteration 1**: Basic text features (section 1 below) - simple patterns to establish baseline pipeline
**Iteration 2**: Domain-specific features (section 2 below) - targeted rules for universities and companies
**Iteration 3**: Character n-grams (section 3 below) - capture subword patterns with TF-IDF
**Iteration 4**: Semantic embeddings (section 4 below) - deep semantic understanding via pre-trained models

I think that the features from iteration 1 are quite naive, and could potentially contribute to
false positives/false negatives. However, I decided to keep them as they improved overall performance
when combined with the more sophisticated features from later iterations. 

## Architecture

```
Input Name → Text Cleaning → Feature Extraction → Random Forest → Classification
                                    ↓
                         449 Features Total:
                         - 9 basic text features
                         - 6 domain-specific features
                         - 50 character n-gram features
                         - 384 semantic embedding features
```

## Feature Engineering

### 1. Basic Text Features (9 features)
- **Length metrics**: Character count, word count
- **Pattern detection**: Prefixes (Dr., Prof.), suffixes (Ltd, Inc, Corp)
- **Character analysis**: Uppercase flag, numbers, special characters
- **Vowel patterns**: Starts/ends with vowel

### 2. Domain-Specific Features (6 features)
- **University keywords**: Detects "university", "college", "institute", "school", "academy"
- **Company suffixes**: Extended detection for "pty", "ltd", "inc", "corp", "llc", "limited", "plc", "gmbh"
- **Acronym patterns**: Consecutive capitals (IBM, UCLA, MIT)
- **Capital ratio**: Proportion of uppercase letters
- **Word length patterns**: First/last word length (e.g., "The University", "Smith LLC")

### 3. Character N-gram TF-IDF (50 features)
- **N-gram range**: 3-4 character sequences
- **Purpose**: Captures subword patterns ("niv" in University, "Inc" in companies)
- **Advantage**: Works for abbreviations and cross-language names
- **Constraint**: Limited to 50 features for memory efficiency

### 4. Semantic Embeddings (384 features)
- **Model**: `all-MiniLM-L6-v2` from sentence-transformers
- **Dimension**: 384-dimensional dense vectors
- **Purpose**: Capture semantic meaning beyond surface-level patterns
- **Advantage**: Understands context and meaning (e.g., "Tech Corp" vs "University of Tech")
- **Implementation**: Pre-trained model locally cached for fast inference
- **Trade-off**: Increases feature space significantly but improves semantic disambiguation 

## Model Selection: Random Forest

I kept it simple, and compared the Random Forest Algorithm to the Logistic Regression Algorithm.
I chose these two models as they both offer some level of interpretability, and they are relatively lightweight models, to ensure that we keep to our resource constraints.

## Performance Metrics

```
Champion Model: Random Forest (Tuned)
├─ Test Accuracy: 93.14%
├─ Test F1-Score: 0.9248
├─ Training Accuracy: 94.41%
├─ Overfitting Gap: 1.27% (excellent generalization)
└─ Per-Class Performance:
   ├─ Person: 99.6% precision
   ├─ University: 95.0% precision
   └─ Company: 60.3% precision
```

## Model Registry & Champion Selection

The system uses a **model registry** (`models/model_registry.json`) to track all trained models:

- Automatically logs metrics for each trained model
- Selects champion based on test F1-score
- Enables easy model comparison and rollback

I chose F1 score as the evaluation metric for this system because it suitably considers both
False Positives and False Negatives. Again, I did not want to over-engineer the system, so
I kept my choice of evaluation metric simple. 

## Future Improvements

Potential enhancements for higher accuracy:

1. **Active learning**: Label misclassified examples to improve Company class (currently 60.3%)
2. **Ensemble methods**: Combine Random Forest with Gradient Boosting. Or perhaps a more complex model?
3. **Domain expansion**: Add features for detecting founders' names in companies
4. **Cross-validation tuning**: Further hyperparameter optimisation
5. **Data augmentation**: Generate synthetic training examples for underrepresented classes
6. **Better Evaluation Metrics**: Come up with a better evaluation metric, perhaps one that would consider the class imbalance?
7. **Company Class**: The model has not learned a robust representation of the 'Company' class and often gets it wrong.